{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK3JkK6nEzFL"
      },
      "source": [
        "# core\n",
        "\n",
        "> This is the core module that will include everything needed for the semantic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCUB2YimEzFN"
      },
      "outputs": [],
      "source": [
        "#| default_exp core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLGKvUzfEzFO"
      },
      "outputs": [],
      "source": [
        "#| hide\n",
        "from nbdev.showdoc import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading parameters"
      ],
      "metadata": {
        "id": "ae6E00ncLNfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want the check the code you'll need your Huggingface tokens. You can do it using login or by loading the tokens from a file.\n",
        "\n",
        "My tokens are in a jason file with is loadded to a Parameters class"
      ],
      "metadata": {
        "id": "IsxAs-ZiLboz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This is for colab integration - uncomment the lines below\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OejKXXtVNbPH",
        "outputId": "7aa05d98-e068-42a5-9b4b-1586eeb270ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_JJULbkREzFP"
      },
      "outputs": [],
      "source": [
        "from reinautils import Parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params=Parameters().from_json ('/content/drive/MyDrive/tokens.json')"
      ],
      "metadata": {
        "id": "VpPlDVMZMQvN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets do some imports"
      ],
      "metadata": {
        "id": "3mbjYt0jOXR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict, Set, Union, Callable\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset, load_dataset\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "import torch.nn.functional as F\n",
        "import transformers"
      ],
      "metadata": {
        "id": "YW3atVtBOQXT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"True\""
      ],
      "metadata": {
        "id": "FEy_GCl9OM0V"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a function for the data preprocessing"
      ],
      "metadata": {
        "id": "Kckk3vAlO1as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "def preprocess_data(dataset: Dataset, splits: Union[str, List[str]] = None, schema: str = \"\") -> Dataset:\n",
        "    \"\"\"\n",
        "    Preprocesses the dataset by merging selected keys into a formatted string.\n",
        "    \n",
        "    Args:\n",
        "        dataset: A HuggingFace Dataset.\n",
        "        splits: The specific splits of the dataset to preprocess. Defaults to all splits.\n",
        "        schema: A string defining how to format the merged string. \n",
        "                It should contain keys from the dataset encapsulated in {}.\n",
        "                Example: \"<human>:{user} <bot>:{response}\", \n",
        "                where 'user' and 'response' are keys in the dataset.\n",
        "\n",
        "    Returns:\n",
        "        The processed Dataset with an additional \"_merged\" field containing the formatted strings.\n",
        "    \"\"\"\n",
        "\n",
        "    # If no splits are specified, use all splits\n",
        "    if not splits:\n",
        "        splits = list(dataset.keys())\n",
        "\n",
        "    # Ensure 'splits' is a list\n",
        "    if not isinstance(splits, (list, tuple)):\n",
        "        splits = [splits] \n",
        "\n",
        "    # If no schema is specified, use a default schema that includes all keys\n",
        "    if not schema:\n",
        "        schema = \"\".join([f\"<{key}>: {{{key}}} \" for key in dataset[splits[0]].features.keys()])\n",
        "\n",
        "    # Extract key names from the schema\n",
        "    key_names = [s.split(\"}\")[0] for s in schema.split(\"{\")[1:]]\n",
        "\n",
        "    # Define a function to merge the columns into a single string\n",
        "    def merge_columns(example):\n",
        "        example[\"_merged\"] = schema.format(**{key: example[key] for key in key_names})\n",
        "        return example\n",
        "    \n",
        "    # Apply the function to the selected splits\n",
        "    for split in splits:\n",
        "        dataset[split] = dataset[split].map(merge_columns)\n",
        "        \n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "9n_LmTwpO0Ad"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a function to compute the embeddings"
      ],
      "metadata": {
        "id": "w-N4YSMPPJK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "def compute_embeddings(\n",
        "    data: Dataset, \n",
        "    embedding_model: torch.nn.Module,\n",
        "    tokenizer, \n",
        "    batch_size: int = 8,\n",
        "    num_workers: int = 1,\n",
        "    dataset_feature : str = '_merged'\n",
        ") -> np.array:\n",
        "    \"\"\"\n",
        "    Compute sentence embeddings using an embedding model.\n",
        "\n",
        "    Args:\n",
        "        data: A list of dictionary containing tokenized text.\n",
        "        embedding_model: A callable model that returns embeddings for input tokens.\n",
        "        batch_size: The number of samples per batch.\n",
        "        num_workers: The number of worker processes for data loading.\n",
        "        dataset_feature : The name of the feature to tokenize in the dataset\n",
        "    Returns:\n",
        "        A numpy array of embeddings for the input data.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(data, batch_size=batch_size, num_workers=num_workers)\n",
        "    embeddings_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            tokenized_batch = tokenizer(batch[dataset_feature], padding='max_length', return_tensors=\"pt\")\n",
        "\n",
        "            # Find the maximum length of tokens in the current batch\n",
        "            max_len = tokenized_batch['attention_mask'].sum(1).max()\n",
        "\n",
        "            # Trim input tensors to the max length\n",
        "            input_ids = tokenized_batch['input_ids'][:, :max_len].to(embedding_model.device)\n",
        "            attention_mask = tokenized_batch['attention_mask'][:, :max_len].to(embedding_model.device)\n",
        "\n",
        "            # Compute embeddings\n",
        "            model_output = embedding_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Average pooling and L2 normalization\n",
        "            sentence_embeddings = mean_pooling(model_output, attention_mask)\n",
        "            sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "\n",
        "            # Store embeddings\n",
        "            embeddings_list.append(sentence_embeddings.to('cpu').numpy())\n",
        "\n",
        "    # Clear CUDA memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return np.concatenate(embeddings_list, 0)\n"
      ],
      "metadata": {
        "id": "rTtnIBEpOz3b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This function will do the deduplication"
      ],
      "metadata": {
        "id": "gqpGMi-BPocG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "def deduplicate_embeddings(embedded, epsilon=1e-2, batch_size=20000):\n",
        "    \"\"\"\n",
        "    Perform deduplication on the provided embeddings.\n",
        "\n",
        "    Args:\n",
        "        embedded: A numpy array or PyTorch tensor holding the embeddings.\n",
        "        epsilon: The maximum distance for two embeddings to be considered duplicates (using cosine similarity).\n",
        "        batch_size: The size of the batches to process at a time.\n",
        "\n",
        "    Note: The embeddings must be L2 normalized.\n",
        "\n",
        "    Returns:\n",
        "        A tensor of indices that should be deleted due to duplication.\n",
        "    \"\"\"\n",
        "\n",
        "    to_delete = torch.empty(0, dtype=int)\n",
        "    embedded_tensor = torch.tensor(embedded, dtype=torch.float16, device='cuda', requires_grad=False)\n",
        "\n",
        "    for i in range(embedded.shape[0]//batch_size+1):\n",
        "        \n",
        "        # Calculate the cosine distance within the current batch\n",
        "        cosine_dist = 1 - torch.matmul(embedded_tensor[i*batch_size:(i+1)*batch_size],\n",
        "                                        torch.transpose(embedded_tensor[i*batch_size:(i+1)*batch_size], 0, 1))\n",
        "        \n",
        "        cosine_dist = cosine_dist + torch.eye(cosine_dist.shape[0],device='cuda')\n",
        "        \n",
        "        # Find duplicate indices within the batch\n",
        "        dup_indices = torch.where(cosine_dist < epsilon)\n",
        "        to_delete = torch.cat((to_delete, dup_indices[0][torch.where(dup_indices[0] > dup_indices[1])].to('cpu') + (i*batch_size)))\n",
        "\n",
        "        # Find duplicate indices across the current batch and remaining batches\n",
        "        for k in range(i+1, embedded.shape[0]//batch_size+1):\n",
        "            cosine_dist = 1 - torch.matmul(embedded_tensor[i*batch_size:(i+1)*batch_size],\n",
        "                                            torch.transpose(embedded_tensor[k*batch_size:(k+1)*batch_size], 0, 1))\n",
        "\n",
        "            dup_indices = torch.where(cosine_dist < epsilon)\n",
        "            to_delete = torch.cat((to_delete, dup_indices[1].to('cpu') + k*batch_size))\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return to_delete"
      ],
      "metadata": {
        "id": "5tjWKiXZPkza"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And in this function we will combine everythin"
      ],
      "metadata": {
        "id": "07osxZWaP49b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "def deduplicate_dataset(\n",
        "    dataset: Dataset, \n",
        "    model: torch.nn.Module, \n",
        "    tokenizer,\n",
        "    epsilon: float = 1e-2, \n",
        "    model_batch_size: int = 64, \n",
        "    deduplication_batch_size: int =20000, \n",
        "    num_workers: int = 16,\n",
        "    dataset_feature: str = '_merged'\n",
        ") -> Dataset:\n",
        "    \"\"\"\n",
        "    Deduplicate data in a dataset based on the embeddings computed by a given model.\n",
        "\n",
        "    Args:\n",
        "        dataset: Dataset to be deduplicated.\n",
        "        model: Model to compute embeddings.\n",
        "        epsilon: Threshold for cosine similarity to consider embeddings as duplicates.\n",
        "        model_batch_size: Batch size for the model.\n",
        "        deduplication_batch_size: Batch size for deduplication process.\n",
        "        num_workers: Number of worker processes for data loading.\n",
        "        dataset_feature: Feature in the dataset to use for deduplication.\n",
        "\n",
        "    Returns:\n",
        "        Deduplicated dataset.\n",
        "    \"\"\"\n",
        "    # Compute embeddings for the dataset\n",
        "    embeddings = compute_embeddings(dataset, \n",
        "                                    model, \n",
        "                                    tokenizer,\n",
        "                                    batch_size=model_batch_size, \n",
        "                                    num_workers=num_workers, \n",
        "                                    dataset_feature=dataset_feature)\n",
        "    \n",
        "    # Find duplicate indices in the embeddings\n",
        "    duplicate_indices = deduplicate_embeddings(embeddings, epsilon, deduplication_batch_size)\n",
        "    \n",
        "    # Filter out duplicate instances from the dataset\n",
        "    deduplicated_dataset = dataset.filter(lambda example, idx: idx not in duplicate_indices, with_indices=True)\n",
        "\n",
        "    return deduplicated_dataset\n",
        "  "
      ],
      "metadata": {
        "id": "O9k63HEtOzqT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's test it all together"
      ],
      "metadata": {
        "id": "wNNsBJlpQYqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and preprocess the data"
      ],
      "metadata": {
        "id": "FR0AzJLNQd9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will do the test using a dataset from Huggingface : [0-hero/OIG-small-chip2](https://huggingface.co/datasets/0-hero/OIG-small-chip2)"
      ],
      "metadata": {
        "id": "F_vNBPd6RPg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = load_dataset(\"0-hero/OIG-small-chip2\")\n",
        "_ = preprocess_data(data,schema = \"<human>:{user} <bot>:{chip2}\")\n",
        "data['train']['_merged'][0]"
      ],
      "metadata": {
        "id": "SKlXBgo5QVKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the tokenizer and model"
      ],
      "metadata": {
        "id": "qid-7QkvQvAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a model for the semantic embedding we'll use [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)"
      ],
      "metadata": {
        "id": "kGBCu7uCQy_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2').to('cuda')"
      ],
      "metadata": {
        "id": "C48-0rqcQxkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run De-duplication"
      ],
      "metadata": {
        "id": "K_1viARCRk_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deduplicated = deduplicate_dataset(\n",
        "    dataset = data['train'], \n",
        "    model = model, \n",
        "    tokenizer = tokenizer,\n",
        "    epsilon = 1e-2, \n",
        "    model_batch_size = 64, \n",
        "    deduplication_batch_size = 20000, \n",
        "    num_workers = 16,\n",
        "    dataset_feature = '_merged'\n",
        ")"
      ],
      "metadata": {
        "id": "_3Oup8qjQt6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the precentage of the dataset we cleaned"
      ],
      "metadata": {
        "id": "TuL_g142Rv8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-len(deduplicated)/len(data['train'])"
      ],
      "metadata": {
        "id": "IewkkX9tR0u7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU_wAtwjEzFP"
      },
      "outputs": [],
      "source": [
        "#| hide\n",
        "import nbdev; nbdev.nbdev_export()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}